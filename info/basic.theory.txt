Node is about scale and speed of development.
To build highly-scalable (non blocking, async nature), data-intensive, and real-time apps.
Node uses Javascript - a lot of programmers know it.
Open-source libraries, you don't have to rebuild a lot of things.

Theory.
Every Browser has JS Engine, it takes JS Code and compiling a machine code from it, so computer can run it.
Edge has chakra (chromium for now), FireFox - SpiderMonkey (Gecko), Chrome V8 (Blink as rendering engine),
Safari - WebKit, Opera - Presto (chromium for now)

Wait what’s Blink? It’s a bit confusing, as Blink is Google Chrome’s rendering engine, V8 is the JavaScript Engine used
within Blink, and Chromium is the browser project that holds them all together. Add a bit of Google product integration
and Chromium becomes Google Chrome.
V8 knows nothing about the DOM, the Document Object Model, as it is meant to just process the JavaScript. Blink
contains the layout engine that decides how to display a site. It makes sense that Node.js would only need V8 and not
Blink, because Node doesn’t need to know anything about the DOM.
Since V8 is written in C++ it compiles JavaScript directly to native, optimized, machine code before executing it,
instead of interpreting it in real time, which is what allows the Node.js to be so speedy and competitive in the server
marketplace.

Because of various engines - JS code can behave differently in browsers.

In 2009 Ryan Dahl, created a Node.js
Node is not a programming language, so it's wrong to say "I prefer C# TO Node", and it's not a framework, so we cannot
say "I prefer ASP.NET to Node", Node is a runtime environment for executing javascript code.
Runtime environment - it's a environment where code is running.

The main idea of single-thread non-blocking work - is thread can working with multiple requests, depending their
ready to be worked with or not.
e.g. Main thread get's a request to DB, it passes the request to DB, and work with next request, while first is
calculating. When DB calculates the data and return response - it's stored in event (callback) queue, thread when it
has no other work to do - picks a nearest to the queue exit callback (with DB data), and put's it in main thread.

Since Node is single threaded, it should not be used for application that needs to calculate some big CPU intensive
operations, because while processor calculates this operation for some request - other should wait for it. So the main
aim is data intensive and real-time applications.

One of the key concept in Node - is events.
e.g. We have a HTTP class, and saying we server listening some port, as soon as machine has new connection to that
port - HTTP class raises the event - "New Request", and we can catch it with handlers.

Express framework is build on top of Http module in node.

In server route /resources/id, we can have a multiple parameters /resources/id/subId they are for essential or
required values, and we have a query parameters /resources/id?sortBy=name - for optional things, that passing some
extra data to server.

With out express.json(), or body-parser - express doesn't see JSON objects that we are sending to it.

TIP: kill process on windows: netstat -ano | findstr :PORT_NUMBER && taskkill /PID YOUR_PID_HERE /F

Note about curl
curl --data '{"name":"Forth Course"}' --header "Content-Type: application/json" --request POST http://localhost:3000/api
curl -d "param1=value1&param2=value2" -H "Content-Type: application/x-www-form-urlencoded" -X POST http://localhost:3000/blahblah

Middleware.
The main idea of middleware, or middleware function that it takes a client request and return an response, or passes
it to the next middleware function. app.get('/', (req, res) => {}) - route handler - is a middleware function, or
app.use(express.json()) - express.json returns a middleware function, that takes a request and if request body is
a json string - it parses it to json object - and add a "body" property with this object to request, and returns it.
When request goes to the server - it's get invoked in request processing pipeline, in this pipeline we have one or more
middlewares, that either terminates pipeline by returning response, or passes request to another middleware.
We can use express.Router - to simplify paths.

MONGO
Install mongodb on mac:
$> brew install mongodb
$>sudo mkdir -p /data/db (We need to create storage for it)
$> sudo chown -R `id -un` /data/db (Set wright permissions to this folder)
$> mongod (Start daemon on mongodb, will start service on 27017)

Go to mongodb.com to download the client for it, called compass.

For Windows - download mongodb installer and client.
Add C:\Program Files\MongoDB\Server\4.2\bin to "path" system variables.
Create storage folder. oleh> md c:\data\db
run from terminal oleh> mongod

So Collection in mongo - same relation DB table
Documents in Collection - same as relation DB rows in table

In mongoose - a client to manage mongo - we have Schemas. In Scheme - we explain em... scheme of Document we want
to create in Collection. From Schema we can create a model, like a class which can create a document objects
in collection.
const humanScheme = new mongoose.Schema({name: String, age: Number}); - created schema for collection
const Human = mongoose.model('HumanCollection', humanScheme); - created model for collection
const human1 = new Human({name: "John", age: 18}) - created object to store in collection
await human1.save() - created document in 'HumanCollection'

Human.find() // filtering
	.limit(10) // limitation of result
	.sort({name: 1}) // sorted by name, -1 - means descending (по убыванию), 1 - ascending (по возрастанию)
	.select({name: 1, tags: 1}) // choose properties that we want to return from object.
	.find({name: /^super/ig}) // using RegExp
	.countDocuments() // to get number of found documents
	.skip(2) // means skip first two documents. Useful if pagination needed, works with limit e.g. .skip(2).limit(10)

So Schema is like class for
In Schema we can use types:
String
Number
Date
Buffer
Boolean
ObjectID - uniq identifier
Array

Comparing operators in mongoose
eq (equal)
ne (not equal)
gt (greater than)
gte (greater than or equal)
lt (less then)
lte (less then or equal)
in
nin (no in)
Genre.find({name: {$gt: 10}, someNumber: {$in: [10, 15, 20]}})

Logical operators
or
and
Genre.find().or([{name: 'This name'}, {isPublished: true}]).and([{name: 'This name'}, {isPublished: true}])

A few examples of queries
GenreModel.find({isPublished: true, tags: 'backend'}).sort({name: 1}).select('name author');
GenreModel.find({isPublished: true, tags: {$in: ['frontend', 'backend']}}).sort('-price').select('name author');
GenreModel.find({isPublished: true}).or([{name: /.*by.*/ig}, {price: {$gte: 15}}]).select('name author price');

Update document
Two ways to do that.
First - query first - means find the document, update it's fields, and save it.
 With this approach we can validate the data that we want to add to document, or check some stuff.
 Good when we get something from client, and want to add this to DB
 client.mongoose.Types.ObjectId.isValid(req.params.id) -> useful check
 const course = await GenreModel.findById({_id: id});
 course.set({key: value}) - to update, or you can update properties of object directly.
 course.save()
Second - update directly in DB
 This approach good if we know what we are doing, and don't need no extra check before update.
 We can do it directly in DB. We can use mongodb update operators "$".
 const resultOfUpdate = await GenreModel.update({_id: id}, {$set: {name: 'New Name'}});
 No need to save it.

 To get a doc that was updated:
 const course = await GenreModel.findByIdAndUpdate(genreId, {$set: {name: 'New Name'}}, {new: true});
 this way - course will be updated object.

Remove document
 const result = await GenreModel.deleteOne({_id: id}); find first doc and delete
 const result = await GenreModel.deleteMany({isPublished: false}); find by query docs and delete
 To get a doc that was deleted:
 const result = await GenreModel.findAndRemove(id);

Validation in mongodb
By default all data in schema is optional, so we cat crate a empty document, which is not good.
We can make a validation for creation, PAY ATTENTION - this validation rules in mongoose level, mongodb directly doesn't
give a ... about it.
const Course = new mongoose.Schema({name: {type: String, required: true}})
const course = new Course({});
await course.validate() -> we'll get an error or
await course.save() -> we'll get an error

Condition validation
const Course = new mongoose.Schema({inStock: Boolean, isPublished: {
        type: Boolean,
        required: function () { return this.inStock; } // if inStock - true, then isPublished - is a required field.
    }
  })

Validators of types
const Course = new mongoose.Schema({
    someStringProp: {
        type: String,
        required: true,
        minlength: 5,
        maxlength: 255,
        match: /^/b.*/d$/ig,
        enum: ['thisOne', 'orThisOne', 'AndThat\'sIt'],
        lowercase: true // means convert passed value to lower case, uppercase? is also available
				trim: true // you got it.
    },
    someNumber: {
            type: Number,
            required: true,
            min: 5,
            max: 200,
            get: v => Math.round(v) // custom getter of value, when value is ejecting from DB - it will be round
            set: v => Math.ceil(v) // custom setter of value, when value is putting to DB - it will be cailed
        }
  })

Custom Validators
const Course = new mongoose.Schema({
    someArrProp: {
        type: Array,
        required: true,
        validate: {
            validator: function(v){
                return v !== null && v.length > 0; // if you don't pass a someArrProp, mongodb will initiate
            },                                     //  an empty array since this is described in scheme
            message: 'someArrProp should be not empty array'
        }
    }
  })

Async validation
const Course = new mongoose.Schema({
    someArrProp: {
        type: Array,
        required: true,
        validate: {
            isAsync: true,
            validator: function(v, cb){
                checkValueInExternalService(v) // returns boolean
                    .then(valid => cb(valid), (notValid) => cb(notValid))
            },
            message: 'someArrProp should be not empty array'
        }
    }
  })

We can iterate on erroneous fields in catch block. e.errors has all fields that was not correctly passed to DB.
for(const field in e.errors){log.error(field)}

References in Mongo DB
Documents usually related to each other.
For example Courses has author field. But Author is not just string, it's a big object with name, website, other
courses, photos and so on. So it's deserves to have separate document or collection, but how we can "relate" courses
and authors in no relational (no SQL) database?

There three ways to connect documents in mongo.
1. Using References (Normalization)
We create a separate document for author:
let author = {_id: 1, name: String, otherInfo: 'bla'}
and "relate" it with course document via _id of author
let course = {author: '1', authors: ['1', '2'], courseInfo: String}

Keep in mind that we can set even invalid id, mongo doesn't have any real connection between those two documents, as it
could be in SQL relational DBs. We using reference "ID" of object we want to add here, that's why it's called "using
reference"

The strengths of this approach is CONSISTENCY. Mean if we want to change schema of author document - we will modify
only one place, and all objects that referenced on authors are get the last version of schema immediately, since they
reference them via ids.
Weakness - is that to run query to get course - we need to run extra query to the DB to get authors, and all others
documents that course object reference to, so this can take some time.

2. Using Embedded Documents (Denormalization)
Instead of creation separate document for author we can create a document inside a document. We can store author object
inside of course object.
let course = {author: {_id: 1, name: String, otherInfo: 'bla'}}

The strengths of this approach is QUERY PERFORMANCE. We need only one query to get full object.
Weakness - if we need to change something - we need to change all objects in collection, and this can cause problems,
errors which brings us to inconsistent data, when something is changed, but something is still old.

3. Hybrid approach. We store object in object, but with reference to another object, to not store all properties from it
but have ability to get them. e.g. From author object in course object we need only name. So we adding a name, but we
want to have ability to check otherInfo if needed, so we adding a reference.
let author = {_id: 1, name: String, otherInfo: 'bla'}
let course = {author: {_id: 1, name: String}, courseInfo: String}

Implementation of those methods looks like this:
===================================
1. Using References (Normalization)
So we have a separate author model
const AuthorModel = mongoose.model('Author', new mongoose.Schema({name: String, website: String}))
const author = new AuthorModel({name: 'John', website: 'www.x.com'}) // {_id: 123, name: 'John', website: 'www.x.com'}

And have a course model
const CourseModel = mongoose.model('Course', new mongoose.Schema({name: String, author: {
        type: mongoose.Schema.Types.ObjectId, // to secure ourselves from adding some not valid id
        ref: 'Author' // this is reference to Author model
    }}))
const course = new CourseModel({name: 'Node Course', author: '123'}).save() -> {_id: 112233, name: 'Node Course', author: 123}

So now we have a course with reference to an author. But if we select courses - we'll get only ids instead of author
objects.
CourseModel.find().select('name author') -> {_id: 112233, name: 'Node Course', author: 123}

To run this extra query - we need "populate" method.
CourseModel.find()
    .populate('author')
    .select('name author') -> {_id: 112233, name: 'Node Course', author: {_id: 123, name: 'John', website: 'www.x.com'}}
And voila, we have full course object with author object.

We specify what property of course model is a reference.
Since we explained in CourseModel that ref: 'Author' model, we don't need to pass a second argument to populate
method, but if we didn't do that we could .populate({path: 'author', model: 'Author'(?)}) -> means what property and
what collection we need to fetch with this id. Or three layered population
.populate({path: 'author', populate: {path: 'friends'}}) - means get author, and get friends of author
Or we can specify property that we want to get .populate('author', 'name'}) and we'll get only authors name in object
Or we can exclude property that we don't want to get .populate('author', 'name -_id'}) and we'll get only authors name
 in object without populated _id

===================================
2. Using Embedded Documents (Denormalization)
const authorSchema = new mongoose.Schema({name: String, website: {type: String, required: true}})
const AuthorModel = mongoose.model('Author', authorSchema)

const CourseModel = mongoose.model('Course', new mongoose.Schema({name: String,
 author: {
 		type: authorSchema, // this is just an example that we can use validation in embedded docs also
 		required: true
	}
}))
const course = new CourseModel({
	name: 'Node Course',
	author: new AuthorModel({name: 'John', website: 'www.x.com'})
}).save() -> {_id: 112233, name: 'Node Course', author: {_id: 123, name: 'John', website: 'www.x.com'}}

So we are creating an object inside object. Problem is that we cannot change author without changing the course.
To change something in author property we can:
const course = await CourseModel.findById(courseId)
course.author.name = 'New Name'
await course.author.name.save()

Or do it directly in DB
const course = await CourseModel.update({_id: courseId}, {
	$set: { // To remove property we can use $unset operator
		'author.name': 'New Name'
	}
})

Make an array or related objects.
const authorSchema = new mongoose.Schema({name: String, website: {type: String, required: true}})
const AuthorModel = mongoose.model('Author', authorSchema)

const CourseModel = mongoose.model('Course', new mongoose.Schema({ name: String, authors: [authorSchema] }))
const course = new CourseModel({
	name: 'Node Course',
	authors: [
		new AuthorModel({name: 'John', website: 'www.x.com'}),
		new AuthorModel({name: 'Bob', website: 'www.y.com'}),
	]
}).save()

To add some object to array - you can just "push" it to array and save it.
await (await CourseModel.findById(courseID)).authors.push(new AuthorModel({name: 'John', website: 'www.x.com'})).save()
To remove item from array - you need id of item
const course = await CourseModel.findById(courseID)
const author = course.authors.id(authorID)
author.remove()
course.save()

======================================================
======================================================
Transactions concept.
In relational DBs we have this concept of transaction. Means group of operations that need to be performed together,
and only if those operations performed successfully - than resource changes applied (commit), if one of the
operations failed, then all changes reverted (rollback) to this transaction reverted and resource isn't changed.
In mongo - there no such thing as transaction - there is some a-like thing like two-phase commit (common protocol for
commit changes in DB):
 Phase 1 - Each server that needs to commit data writes its data records to the log. If a server is unsuccessful,
  it responds with a failure message. If successful, the server replies with an OK message.
 Phase 2 - This phase begins after all participants respond OK. Then, the coordinator sends a signal to each server
  with commit instructions. After committing, each writes the commit as part of its log record for reference and sends
  the coordinator a message that its commit has been successfully implemented. If a server fails, the coordinator
  sends instructions to all servers to roll back the transaction. After the servers roll back, each sends feedback
  that this has been completed.

In package "fawn" there is ability to do "transactions" in mongodb. Ability to do that implemented above two-phase
commit technique. Also it creates an extra collection to manage these two-phase commits stuff. Looks like a crutch to me.

==================================================
ObjectID in mongodb
_id: 5b77fdc3215eda645bc6bdec
12 bytes
first 4 bytes - timestamp, so great thing that we don't need to save date of data creation - it's included in id.
next 3 bytes - machine identifiers, individual for machine.
next 2 bytes - process identifiers, individual for process.
last 3 bytes - counter of creation.

12 bytes of data. 96 bits (1 byte - 8 bits). 1 bit can hold 0 or 1, means 1 byte can hold 2 ^ 8 = 256 numbers.
Counter is 3 bytes, means 2 ^ 24 = 16 millions.
So if on same machine, on same process, on same time - we'll create more that 16 millions commits in db - ID will be
duplicated :)

ObjectID generated not by mongodb, but by mongodb driver.
Mongoose is over mongodb driver.
We can generate an ObjectID with mongoose.
const id = new mongoose.Types.ObjectId()
To get timestamp
const timestamp = id.getTimestamp();

=============================
Hash.
To hash a password we need a salt.
For pass 1234 -> we hash it to adfc, so if hacker will get this data - it couldn't decrypt it to 1234.
But he can grab a hashing algorithm and list of popular passwords, hash them and compare with adfc - so he has a chance
to understand which password is hashed. That's why we need salt - it's basically a random string that added to start or
to the end of hashed password, so it's different each time it's hashed.

JSON Web Token
Long string that identifies a User.
When User logged in system - server gives to this client a JWT, client should store it.
Then client should use it in further requests to server.
Web app clients usually store JWT in Local Storage, mobile - similar option but depend on platform.

JWT - is a standard, and we can see on jwt.io what parts it's holding
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. -> HEADER:ALGORITHM & TOKEN TYPE {"alg": "HS256", "typ": "JWT"}
eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI65MDIyfQ. ->
    PAYLOAD:DATA {"sub": "1234567890","name": "John Doe", "admin": true, "iat": "1516239022"}
SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c ->
    VERIFY SIGNATURE: HMACSHA256(base64UrlEncode(header) + "." + base64UrlEncode(payload), your-256-bit-secret)

First part - is a standard headers.
PAYLOAD - main thing, we can extract on client and on server side basic information about the user from it's token.
So we don't need to query each time on the server side, is this user an admin or not, we can get this info from token
that he passes to us. "iat" - token creation time, we can understand age of token, to update it.
We can think that any user can generate a JWT with admin rights, but this third part is made to secure this door.
SIGNATURE - is a digital signature made with PAYLOAD and secret private key that stored on the server. So if anyone
changes the payload, and tries to generate new JWT - digital signature will be spoiled, and server will understand it,
and decline it with "Not valid token".
So to fake JWT - you need to have access to private key.

Tokens must be stored on client, and we don't need to store or delete tokens from backed. So when user is logout - we
should delete token on client side. So User will be forced to login again and get token.
Storing tokens in plain text in DB - same as storing passwords. If you really need to save tokens in DB - hash them.
When you sending token to client, use https - to not get criminal to own via sniffing your users tokens.

=========================================
Winston logger - has ability to log message in console, file, or http endpoint, these things called transport, a place
to log message. Winston can be integrated with mongodb, couchdb, redis, loggly (also a logging service)
So we can create a few instances of winston, and set them different ways to log stuff, or from one logger log to many
things like console, some db, or external web service which is pretty cool.
Winston has levels error warn info verbose debug silly
